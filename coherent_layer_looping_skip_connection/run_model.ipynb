{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Required Libraries and Model\n",
    "Import necessary libraries and load the pretrained model from the saved checkpoint directory. Include error handling for model loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Initialize the accelerator for distributed inference if needed\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Define the path to the saved model checkpoint\n",
    "checkpoint_dir = \"./output/layer_looping_qwen/best_model\"\n",
    "\n",
    "# Load the tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to load tokenizer from {checkpoint_dir}: {e}\")\n",
    "\n",
    "# Load the model\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(checkpoint_dir)\n",
    "    model = accelerator.prepare(model)  # Prepare the model for distributed inference\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to load model from {checkpoint_dir}: {e}\")\n",
    "\n",
    "# Confirm successful loading\n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Tokenizer and Model Configuration\n",
    "Initialize the tokenizer from the same checkpoint, configure generation parameters like max length, temperature, and top_p. Set up the model's layer looping parameters (n, m, max_loop_count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7319/3561659619.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"top_p\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Nucleus sampling probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"do_sample\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Enable sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;34m\"eos_token_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# End-of-sequence token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Set up generation parameters\n",
    "generation_config = {\n",
    "    \"max_length\": 128,  # Maximum length of the generated sequence\n",
    "    \"temperature\": 0.7,  # Sampling temperature\n",
    "    \"top_p\": 0.9,  # Nucleus sampling probability\n",
    "    \"do_sample\": True,  # Enable sampling\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,  # End-of-sequence token\n",
    "}\n",
    "\n",
    "# Configure layer looping parameters\n",
    "layer_looping_config = {\n",
    "    \"n\": 8,  # Start layer index for looping\n",
    "    \"m\": 12,  # End layer index for looping\n",
    "    \"max_loop_count\": 5,  # Maximum number of times to loop\n",
    "}\n",
    "\n",
    "# Add layer looping parameters to the model's configuration\n",
    "if hasattr(model, \"config\"):\n",
    "    model.config.update(layer_looping_config)\n",
    "\n",
    "# Confirm configuration setup\n",
    "print(\"Generation and layer looping configurations set up successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Text Generation Function\n",
    "Implement a helper function that handles input preprocessing, model inference, and output post-processing. Include parameters for controlling the loop count and generation settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for text generation\n",
    "def generate_text(prompt, loop_count=1, max_length=None, temperature=None, top_p=None):\n",
    "    \"\"\"\n",
    "    Generate text using the pretrained layer looping transformer model.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input text prompt for generation.\n",
    "        loop_count (int): Number of times to loop through the specified layers.\n",
    "        max_length (int, optional): Maximum length of the generated sequence. Defaults to the value in generation_config.\n",
    "        temperature (float, optional): Sampling temperature. Defaults to the value in generation_config.\n",
    "        top_p (float, optional): Nucleus sampling probability. Defaults to the value in generation_config.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text.\n",
    "    \"\"\"\n",
    "    # Update generation parameters with provided values or defaults\n",
    "    gen_params = generation_config.copy()\n",
    "    if max_length is not None:\n",
    "        gen_params[\"max_length\"] = max_length\n",
    "    if temperature is not None:\n",
    "        gen_params[\"temperature\"] = temperature\n",
    "    if top_p is not None:\n",
    "        gen_params[\"top_p\"] = top_p\n",
    "\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(accelerator.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(accelerator.device)\n",
    "\n",
    "    # Add loop count to the model's configuration\n",
    "    if hasattr(model, \"config\"):\n",
    "        model.config.k = loop_count\n",
    "\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **gen_params\n",
    "        )\n",
    "        print(outputs)\n",
    "\n",
    "    # Decode the generated tokens\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generation_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7319/2361301135.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The meaning of life is\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_7319/334779681.py\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(prompt, loop_count, max_length, temperature, top_p)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \"\"\"\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Update generation parameters with provided values or defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mgen_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mgen_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generation_config' is not defined"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"The meaning of life is\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Text Generation\n",
    "Create an interactive interface for entering prompts and generating text responses. Include options to adjust generation parameters in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layer_looping_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7319/3873349191.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_looping_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_loop_count\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Loop Count:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layer_looping_config' is not defined"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for interactive widgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Define interactive widgets for text generation\n",
    "prompt_input = widgets.Text(\n",
    "    value=\"\",\n",
    "    placeholder=\"Enter your prompt here...\",\n",
    "    description=\"Prompt:\",\n",
    "    layout=widgets.Layout(width=\"100%\")\n",
    ")\n",
    "\n",
    "loop_count_slider = widgets.IntSlider(\n",
    "    value=1,\n",
    "    min=1,\n",
    "    max=layer_looping_config[\"max_loop_count\"],\n",
    "    step=1,\n",
    "    description=\"Loop Count:\",\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "max_length_slider = widgets.IntSlider(\n",
    "    value=generation_config[\"max_length\"],\n",
    "    min=16,\n",
    "    max=512,\n",
    "    step=16,\n",
    "    description=\"Max Length:\",\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "temperature_slider = widgets.FloatSlider(\n",
    "    value=generation_config[\"temperature\"],\n",
    "    min=0.1,\n",
    "    max=1.5,\n",
    "    step=0.1,\n",
    "    description=\"Temperature:\",\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "top_p_slider = widgets.FloatSlider(\n",
    "    value=generation_config[\"top_p\"],\n",
    "    min=0.1,\n",
    "    max=1.0,\n",
    "    step=0.1,\n",
    "    description=\"Top-p:\",\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "generate_button = widgets.Button(\n",
    "    description=\"Generate\",\n",
    "    button_style=\"success\",\n",
    "    tooltip=\"Click to generate text\",\n",
    "    icon=\"rocket\"\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Define the callback function for text generation\n",
    "def on_generate_button_click(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        prompt = prompt_input.value\n",
    "        loop_count = loop_count_slider.value\n",
    "        max_length = max_length_slider.value\n",
    "        temperature = temperature_slider.value\n",
    "        top_p = top_p_slider.value\n",
    "        \n",
    "        if not prompt.strip():\n",
    "            print(\"Please enter a valid prompt.\")\n",
    "            return\n",
    "        \n",
    "        print(\"Generating text...\")\n",
    "        generated_text = generate_text(\n",
    "            prompt=prompt,\n",
    "            loop_count=loop_count,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p\n",
    "        )\n",
    "        print(f\"Generated Text:\\n{generated_text}\")\n",
    "\n",
    "# Attach the callback to the button\n",
    "generate_button.on_click(on_generate_button_click)\n",
    "\n",
    "# Display the interactive interface\n",
    "display(\n",
    "    widgets.VBox([\n",
    "        prompt_input,\n",
    "        loop_count_slider,\n",
    "        max_length_slider,\n",
    "        temperature_slider,\n",
    "        top_p_slider,\n",
    "        generate_button,\n",
    "        output_area\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with Different Loop Counts\n",
    "Compare model outputs with different loop counts (k values) using the same input prompt. Analyze how the number of loops affects generation quality and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with Different Loop Counts\n",
    "\n",
    "# Define a function to compare outputs with different loop counts\n",
    "def compare_loop_counts(prompt, loop_counts, max_length=None, temperature=None, top_p=None):\n",
    "    \"\"\"\n",
    "    Compare model outputs with different loop counts using the same input prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input text prompt for generation.\n",
    "        loop_counts (list): List of loop counts to compare.\n",
    "        max_length (int, optional): Maximum length of the generated sequence. Defaults to the value in generation_config.\n",
    "        temperature (float, optional): Sampling temperature. Defaults to the value in generation_config.\n",
    "        top_p (float, optional): Nucleus sampling probability. Defaults to the value in generation_config.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping loop counts to generated texts.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for loop_count in loop_counts:\n",
    "        generated_text = generate_text(\n",
    "            prompt=prompt,\n",
    "            loop_count=loop_count,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p\n",
    "        )\n",
    "        results[loop_count] = generated_text\n",
    "    return results\n",
    "\n",
    "# Define widgets for comparison\n",
    "compare_prompt_input = widgets.Text(\n",
    "    value=\"\",\n",
    "    placeholder=\"Enter your prompt here...\",\n",
    "    description=\"Prompt:\",\n",
    "    layout=widgets.Layout(width=\"100%\")\n",
    ")\n",
    "\n",
    "loop_counts_input = widgets.Text(\n",
    "    value=\"1,2,3\",\n",
    "    placeholder=\"Enter loop counts (comma-separated)...\",\n",
    "    description=\"Loop Counts:\",\n",
    "    layout=widgets.Layout(width=\"100%\")\n",
    ")\n",
    "\n",
    "compare_button = widgets.Button(\n",
    "    description=\"Compare\",\n",
    "    button_style=\"info\",\n",
    "    tooltip=\"Click to compare outputs with different loop counts\",\n",
    "    icon=\"search\"\n",
    ")\n",
    "\n",
    "compare_output_area = widgets.Output()\n",
    "\n",
    "# Define the callback function for comparison\n",
    "def on_compare_button_click(b):\n",
    "    with compare_output_area:\n",
    "        compare_output_area.clear_output()\n",
    "        prompt = compare_prompt_input.value\n",
    "        loop_counts = loop_counts_input.value\n",
    "        \n",
    "        if not prompt.strip():\n",
    "            print(\"Please enter a valid prompt.\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            loop_counts = [int(x.strip()) for x in loop_counts.split(\",\") if x.strip().isdigit()]\n",
    "        except ValueError:\n",
    "            print(\"Invalid loop counts. Please enter a comma-separated list of integers.\")\n",
    "            return\n",
    "        \n",
    "        if not loop_counts:\n",
    "            print(\"Please enter at least one valid loop count.\")\n",
    "            return\n",
    "        \n",
    "        print(\"Comparing outputs with different loop counts...\")\n",
    "        results = compare_loop_counts(\n",
    "            prompt=prompt,\n",
    "            loop_counts=loop_counts,\n",
    "            max_length=max_length_slider.value,\n",
    "            temperature=temperature_slider.value,\n",
    "            top_p=top_p_slider.value\n",
    "        )\n",
    "        \n",
    "        for loop_count, text in results.items():\n",
    "            print(f\"\\nLoop Count: {loop_count}\\n{'-' * 20}\\n{text}\")\n",
    "\n",
    "# Attach the callback to the button\n",
    "compare_button.on_click(on_compare_button_click)\n",
    "\n",
    "# Display the comparison interface\n",
    "display(\n",
    "    widgets.VBox([\n",
    "        compare_prompt_input,\n",
    "        loop_counts_input,\n",
    "        compare_button,\n",
    "        compare_output_area\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Processing for Multiple Prompts\n",
    "Implement batch processing functionality to generate responses for multiple prompts simultaneously. Include performance optimization for batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Processing for Multiple Prompts\n",
    "\n",
    "# Define a function for batch processing\n",
    "def generate_batch(prompts, loop_count=1, max_length=None, temperature=None, top_p=None):\n",
    "    \"\"\"\n",
    "    Generate responses for multiple prompts in a batch.\n",
    "\n",
    "    Args:\n",
    "        prompts (list): A list of input text prompts for generation.\n",
    "        loop_count (int): Number of times to loop through the specified layers.\n",
    "        max_length (int, optional): Maximum length of the generated sequence. Defaults to the value in generation_config.\n",
    "        temperature (float, optional): Sampling temperature. Defaults to the value in generation_config.\n",
    "        top_p (float, optional): Nucleus sampling probability. Defaults to the value in generation_config.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of generated texts corresponding to the input prompts.\n",
    "    \"\"\"\n",
    "    # Update generation parameters with provided values or defaults\n",
    "    gen_params = generation_config.copy()\n",
    "    if max_length is not None:\n",
    "        gen_params[\"max_length\"] = max_length\n",
    "    if temperature is not None:\n",
    "        gen_params[\"temperature\"] = temperature\n",
    "    if top_p is not None:\n",
    "        gen_params[\"top_p\"] = top_p\n",
    "\n",
    "    # Tokenize the input prompts\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(accelerator.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(accelerator.device)\n",
    "\n",
    "    # Add loop count to the model's configuration\n",
    "    if hasattr(model, \"config\"):\n",
    "        model.config.k = loop_count\n",
    "\n",
    "    # Generate text in batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **gen_params\n",
    "        )\n",
    "\n",
    "    # Decode the generated tokens\n",
    "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    return generated_texts\n",
    "\n",
    "# Define widgets for batch processing\n",
    "batch_prompts_input = widgets.Textarea(\n",
    "    value=\"Prompt 1\\nPrompt 2\\nPrompt 3\",\n",
    "    placeholder=\"Enter one prompt per line...\",\n",
    "    description=\"Prompts:\",\n",
    "    layout=widgets.Layout(width=\"100%\", height=\"150px\")\n",
    ")\n",
    "\n",
    "batch_loop_count_slider = widgets.IntSlider(\n",
    "    value=1,\n",
    "    min=1,\n",
    "    max=layer_looping_config[\"max_loop_count\"],\n",
    "    step=1,\n",
    "    description=\"Loop Count:\",\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "batch_generate_button = widgets.Button(\n",
    "    description=\"Generate Batch\",\n",
    "    button_style=\"success\",\n",
    "    tooltip=\"Click to generate text for all prompts\",\n",
    "    icon=\"rocket\"\n",
    ")\n",
    "\n",
    "batch_output_area = widgets.Output()\n",
    "\n",
    "# Define the callback function for batch processing\n",
    "def on_batch_generate_button_click(b):\n",
    "    with batch_output_area:\n",
    "        batch_output_area.clear_output()\n",
    "        prompts = batch_prompts_input.value.strip().split(\"\\n\")\n",
    "        loop_count = batch_loop_count_slider.value\n",
    "        max_length = max_length_slider.value\n",
    "        temperature = temperature_slider.value\n",
    "        top_p = top_p_slider.value\n",
    "        \n",
    "        if not prompts or all(not prompt.strip() for prompt in prompts):\n",
    "            print(\"Please enter at least one valid prompt.\")\n",
    "            return\n",
    "        \n",
    "        print(\"Generating text for batch...\")\n",
    "        generated_texts = generate_batch(\n",
    "            prompts=prompts,\n",
    "            loop_count=loop_count,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p\n",
    "        )\n",
    "        \n",
    "        for i, (prompt, text) in enumerate(zip(prompts, generated_texts), 1):\n",
    "            print(f\"\\nPrompt {i}: {prompt}\\n{'-' * 20}\\n{text}\")\n",
    "\n",
    "# Attach the callback to the button\n",
    "batch_generate_button.on_click(on_batch_generate_button_click)\n",
    "\n",
    "# Display the batch processing interface\n",
    "display(\n",
    "    widgets.VBox([\n",
    "        batch_prompts_input,\n",
    "        batch_loop_count_slider,\n",
    "        max_length_slider,\n",
    "        temperature_slider,\n",
    "        top_p_slider,\n",
    "        batch_generate_button,\n",
    "        batch_output_area\n",
    "    ])\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
