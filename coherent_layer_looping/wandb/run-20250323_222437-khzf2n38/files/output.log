/home/niket/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Effective batch size: 32
/home/niket/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Model initialized with looping configuration:
  - Total layers: 24
  - Early layers: 0 to 7
  - Loop layers: 8 to 12
  - Late layers: 13 to 23
  - Max loop count: 10
README.md: 100%|█████████████████████████████████████████████████| 6.10k/6.10k [00:00<00:00, 20.8MB/s]
tiny_shakespeare.py: 100%|███████████████████████████████████████| 3.73k/3.73k [00:00<00:00, 15.2MB/s]
Downloading data: 100%|██████████████████████████████████████████| 1.12M/1.12M [00:00<00:00, 49.9MB/s]
Generating train split: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 62.53 examples/s]
Generating validation split: 100%|██████████████████████████████| 1/1 [00:00<00:00, 150.49 examples/s]
Generating test split: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 161.97 examples/s]
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map: 100%|███████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.30 examples/s]
num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.
Map: 100%|███████████████████████████████████████████████████████| 1/1 [00:00<00:00, 21.22 examples/s]
***** Running training *****
  Num examples = 1
  Num epochs = 3
  Per-device batch size = 4
  Gradient accumulation steps = 8
  Total optimization steps = 0
  Using distillation: True
0it [00:00, ?it/s]Traceback (most recent call last):
  File "/home/niket/3E/train.py", line 475, in <module>
    main()
  File "/home/niket/3E/train.py", line 315, in main
    outputs = model(
              ^^^^^^
  File "/home/niket/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/niket/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/niket/3E/model.py", line 69, in forward
    hidden_states = layer(hidden_states, attention_mask=attention_mask)[0]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/niket/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/niket/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/niket/miniconda3/lib/python3.12/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/niket/miniconda3/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 768, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "/home/niket/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/niket/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/niket/miniconda3/lib/python3.12/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/niket/miniconda3/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 682, in forward
    raise ValueError(
ValueError: Attention mask should be of size (1, 1, 512, 512), but is torch.Size([1, 512])
